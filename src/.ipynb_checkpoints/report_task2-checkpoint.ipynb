{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Learning Based Strategy for Athena"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Athena has various strategies for making a final prediction based on the predictions of the ensemble of weak defenses. These strategies are relatively basic, and leave a large potential for optimization using a neural network. Our strategy involves running the predictions of the ensemble through a neural network trained to make a final prediction based on the predictions of the ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed Rundown of Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Generate data set\n",
    "    1. Need to gather test and training data\n",
    "        - must be uniformly distributed to eliminate bias\n",
    "        - data must be adversarial examples from Athena\n",
    "        - need the proper labels for each example\n",
    "        - need raw predictions from each weak defense for each adversarial example\n",
    "2. Create Neural Network\n",
    "    1. Train a CNN on training data\n",
    "        - Input will be nx10, where n is the number of weak defenses\n",
    "        - Error produced by comparison with 1x10 array in the form [0,0,0,0,x,0,0,0,0,0], where x is the prediction\n",
    "        - Output is the predicted label for the image\n",
    "    2. Implement into Athena\n",
    "        - NN will be a specific strategy that Athena can utilize, must implement this strategy name into main athena file\n",
    "3. Test Neural Network\n",
    "    1. Run previously generated test data through NN, comparing output with true labels\n",
    "    2. Determine error rate of new athena strategy\n",
    "\n",
    "**Important Note:**\n",
    "Our implemetation of this strategy uses 15 randomly selected weak defenses. This is to reduce data collection and training time. With superior computing power, all 72 weak defenses would be uses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Generate Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set was generated using the AE's found in (a); this is all of the AE's included with Athena. A subsample was generated splitting the data from each AE found in (a) into 80% training, 20% testing, using (b). Using (c), we looped through our dataset, generating the raw predictions from each weak defense defined in \"active_wds\" field in (d). Finally, (e) was used to generate the labels for training in a manner which can be used to create an error function for the NN as described in 2A. To transform the test data into a useable form, (f) was used.\n",
    "\n",
    "**Total training samples:** 3600\n",
    "**Total test samples:** 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Files:**\n",
    "- (a): src/configs/experiment/data-mnist.json\n",
    "- (b): src/learning_based_strategy/split_data.py\n",
    "- (c): src/learning_based_strategy/collect_raws.py\n",
    "- (d): src/configs/experiment/athena-mnist.json\n",
    "- (e): src/learning_based_strategy/get_training_labels.py\n",
    "- (f): src/learning_based_strategy/get_test_samples.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relevant Code Snippets for 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Thu Nov 12 17:49:52 2020\n",
    "\n",
    "@author: miles\n",
    "\"\"\"\n",
    "import os\n",
    "from utils.file import load_from_json\n",
    "from utils.data import subsampling\n",
    "import numpy as np\n",
    "\n",
    "data_configs = load_from_json('../configs/experiment/data-mnist.json')\n",
    "\n",
    "path = 'samples/'\n",
    "\n",
    "#get data files, only take the AE type for the filename for matching later\n",
    "data_files = [os.path.join(data_configs.get('dir'), ae_file) for ae_file in data_configs.get('ae_files')]\n",
    "filenames = [ae_file.split('-')[-1].replace('.npy','') for ae_file in data_configs.get('ae_files')]\n",
    "#Get respective label files\n",
    "label_file = os.path.join(data_configs.get('dir'), data_configs.get('label_file'))\n",
    "labels = np.load(label_file)\n",
    "\n",
    "#Subsample from each AE file\n",
    "for file, filename in zip(data_files, filenames):\n",
    "\n",
    "    data = np.load(file)\n",
    "    subsampling(data, labels, 10, 0.2, path, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (c) from collect_raw_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #get samples and respective labels\n",
    "    samples = glob.glob('samples/*training_samples*.npy')\n",
    "    labels = glob.glob('samples/*training_labels*.npy')\n",
    "    \n",
    "    #sort based on type of attack\n",
    "    sorted_samples = []\n",
    "    sorted_labels = []\n",
    "    for sample in samples:\n",
    "        pref = sample.split('_training_samples')[0].split('/')[1].replace('.npy','')\n",
    "        sorted_samples.append(sample)\n",
    "        for label in labels:\n",
    "            pref2 = label.split('_training_labels')[0].split('/')[1].replace('.npy','')\n",
    "            if pref == pref2:\n",
    "                sorted_labels.append(label)\n",
    "    \n",
    "    #load data and labels, concatenate into single numpy array for easy looping\n",
    "    samples_dat = [np.load(dat) for dat in sorted_samples]\n",
    "    labels_dat = [np.load(dat) for dat in sorted_labels]\n",
    "    samples_dat = np.concatenate(samples_dat,axis=0)\n",
    "    labels_dat = np.concatenate(labels_dat)\n",
    "    samples = []\n",
    "    labels = []\n",
    "    #Generate raw predictions from each WD for each AE\n",
    "    for i in range(0, len(labels_dat), 100):\n",
    "        raw_preds = athena.predict(x=samples_dat[i], raw=True)\n",
    "        samples.append(raw_preds)\n",
    "        labels.append(labels_dat[i])\n",
    "\n",
    "    #Write out raw predictions to training_data directory\n",
    "    samples = np.concatenate(samples,axis=1)\n",
    "    labels = np.array(labels)\n",
    "    samples_file = os.path.join('training_data/', 'training.npy')\n",
    "    np.save(file=samples_file,arr=samples)\n",
    "    labels_file = os.path.join('training_data/','training_labels.npy')\n",
    "    np.save(file=labels_file,arr=labels)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Sun Nov 15 20:08:08 2020\n",
    "\n",
    "@author: miles\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "file = 'training_data/training_labels.npy'\n",
    "labels = np.load(file)\n",
    "arr = []\n",
    "for label in labels:\n",
    "    temp = [0,0,0,0,0,0,0,0,0,0]\n",
    "    temp[label] = 1;\n",
    "    arr.append(temp)\n",
    "    \n",
    "new_arr = np.array(arr)\n",
    "np.save(file='training_data/labels2D.npy', arr=new_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Sun Nov 15 17:53:22 2020\n",
    "\n",
    "@author: miles\n",
    "\"\"\"\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "samples = glob.glob('samples/*test_samples*.npy')\n",
    "labels = glob.glob('samples/*test_labels*.npy')\n",
    "sorted_samples = []\n",
    "sorted_labels = []\n",
    "for sample in samples:\n",
    "    pref = sample.split('_test_samples')[0].split('/')[1].replace('.npy','')\n",
    "    sorted_samples.append(sample)\n",
    "    for label in labels:\n",
    "        pref2 = label.split('_test_labels')[0].split('/')[1].replace('.npy','')\n",
    "        if pref == pref2:\n",
    "            sorted_labels.append(label)\n",
    "    \n",
    "samples_dat = [np.load(dat) for dat in sorted_samples]\n",
    "labels_dat = [np.load(dat) for dat in sorted_labels]\n",
    "samples_dat = np.concatenate(samples_dat,axis=0)\n",
    "labels_dat = np.concatenate(labels_dat)\n",
    "samples = []\n",
    "labels = []\n",
    "for i in range(0, len(labels_dat), 90):\n",
    "    samples.append(samples_dat[i])\n",
    "    labels.append(labels_dat[i])\n",
    "labels_file = os.path.join('testing/', 'test_labels.npy')\n",
    "samples_file = os.path.join('testing/', 'test_samples.npy')\n",
    "labels = np.array(labels)\n",
    "samples = np.array(samples)\n",
    "np.save(file=labels_file,arr=labels)\n",
    "np.save(file=samples_file,arr=samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network has an input layer of shape 15x10, (15 weak defenses were used for this model chosen randomly in order to reduce training time, 10 predictions from each weak defense), two hidden layers and an output layer of 10x1. The output still generates 10 values using softmax, but picks the highest value in a postprocessing phase within Athena. The hidden layers take the input to 150x4, then back down to 150. All activations are relu, except for the final layer which is softmax. \n",
    "**NOTE:** This method should be optimized by using all 72 weak defenses, so the input layer should be 72x10. The network should also be trained on many more than 3600 examples. We did not have the necessary resources to do this in a reasonable amount of time\n",
    "**FILES:**\n",
    "- create NN: src/learning_based_strategy/models/nn.py\n",
    "- NN stored in: src/learning_based_strategy/models/learning-strategy-nn.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nn.py\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "training_data = np.load('../training_data/training.npy')\n",
    "training_labels = np.load('../training_data/labels2D.npy')\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(150, activation='relu', input_shape=(15 * 10,)))\n",
    "network.add(layers.Dense(600, activation='relu', input_shape=(150 * 4,)))\n",
    "network.add(layers.Dense(150, activation='relu'))\n",
    "network.add(layers.Dense(10, activation='softmax'))\n",
    "network.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "training_data = np.transpose(training_data, (1,0,2))\n",
    "training_data = training_data.reshape((3600,15*10))\n",
    "\n",
    "network.fit(training_data, training_labels, epochs=10, batch_size=360)\n",
    "network.save('learning-strategy-nn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
